{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "#### Here, we will be looking at a portion of code which scores participant behavioral data collected during my honors thesis project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description\n",
    "\n",
    "#### The overall aim of this project is to characterize the spatiotemporal neural dynamics of statistical learning (SL), a neurocognitive mechanism critical for perceptual learning. Here, participants listen to either structured or un-structured phonological sequences made up of 12 syllables while laying in a MRI scanner. The structure of the sequence is determined by transitional probabilities (TP). \n",
    "\n",
    "#### In the strucutred sequence, three phonemes were paired together to make a \"word\", which is the base unit repeated throughout the sequence, whereas the un-structured sequence contained no \"words\" and all syllables were randomly played, thus the base unit being a single phoneme. This means that the TP across base units in the structured group is $\\frac{1}{3}$[^1] and within base units is $1$, while the TP of the base unit in the un-structured group is $\\frac{1}{11}$. Three decades of evidence have shown that humans are sensitive to these TP's and are able to segment continuous input using the strucutre as defined by the TPs. Below is an illustrative example of the TPs in the structured versus un-structured sequence given *6 syllables and 2 words* (note: the real experiment consists of 12 syllables comprising 4 words).\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Structured TPs\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "    t1((/pu/))-.->|1.0| t2((/bi/))-.->|1.0| t3((/ka/))-.->|0.33| t4((/di/))-.->|1.0| t5((/da/))-.->|1.0| t6((/bu/))\n",
    "    t1---|WORD|t3\n",
    "    t4---|WORD|t6\n",
    "    t6--->|REPEATED|t1\n",
    "    style t1 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t2 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t3 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t4 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t5 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t6 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Un-structured TPs\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "    t1((/pu/))-.->|0.09| t2((/bi/))-.->|0.09| t3((/ka/))-.->|0.09| t4((/di/))-.->|0.09| t5((/da/))-.->|0.09| t6((/bu/))\n",
    "    style t1 fill:#f45,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t2 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t3 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t4 fill:#451,stroke:#333,stroke-width:4px,color:#fff\n",
    "    style t5 fill:#205,stroke:#333,stroke-width:4px,color:#fff\n",
    "    style t6 fill:#904,stroke:#333,stroke-width:4px,color:#000\n",
    "\n",
    "```\n",
    "\n",
    "#### Ultimately, using advanced computational techniques--such as a [Hidden Markov Model (HMM)](https://brainiak.org/tutorials/12-hmm/)--, I expect to uncover three distinct subprocess of SL in the brain: a perceptual, encoding and predictive process.[^2] This would provide a spatially detailed and mechanistic account of SL, thus giving credence to existing evidence positing the compositionality of SL.[^3] Importantly, we must also confirm that participants actually learned (or did not learn) the structure.\n",
    "\n",
    "#### Upon completion of the sequence in the scanner, participants exited the scanner and completed a test which assessed whether an individual learned the structure of the phonological sequence. This test is composed of a three-alternative force choiced task, in which a word from the structured sequence (dubbed \"target word\") is pitted against two other foil words, which have never been heard by the participant. It is the participant's job to discriminate between the foils and target word by correctly selecting the target word. Below is an example trial.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Example trial\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "A[FOIL]-.-> B[TARGET]-.-> C[FOIL]\n",
    "\n",
    "style A fill:#025\n",
    "style B fill:#f45\n",
    "style C fill:#025\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "D[Each presentation is played sequentially with a total of 12 trials.]\n",
    "```\n",
    "\n",
    "#### The code used to determine whether participants reliably learned the structure is reviewed in this notebook. \n",
    "\n",
    " [^1]: The construction of the sequences was constrained such that individual units could not immediately repeat themselves. More on the algorithm used to create sequences and its implementation can be found [here](/Honors-Thesis/README.md).\n",
    " [^2]: Information on the HMM implementation can be found [here](https://github.com/w-decker/SNL23_plots/blob/main/plotting.ipynb).\n",
    " [^3]: See [Batterink & Paller (2017)](https://www.batterinklab.com/_files/ugd/a9b75d_53f0f5269f5942cb81105ef47c84dba5.pdf) and [Moser et al. (2021)](https://www.batterinklab.com/_files/ugd/a9b75d_ab33c519fa7a406e92e68369eacfce2f.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on the behavioral assessment and its analysis\n",
    "\n",
    "#### As seen above, participants had to determine the target word amidst two foil words. To assess learning, I examined whether individuals who were exposed to the structured sequence performed above chance ($0.33$) using a one-tailed one sample t-test and whether this performance was significantly greater than those exposed to the un-structured sequence using a one-tailed independent samples t-test.\n",
    "\n",
    "#### Additionally, this experiment was executed using PsychoPy; the resulting output is convoluted and the actual results of the assessment must be extracted from a file containing highly erroneous (in our particular case) data.\n",
    "\n",
    "#### Therefore, to analyze participant's SL abilities, I created a custom module for specifically handling the PsychoPy behavioral output. This module is located within the repo submodule. For the purposes of this notebook, I'll extract it and bring it over to the current path so as to demonstrate its functionality and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the custom module\n",
    "\n",
    "#### The custom module, named `scoring_module.py`, is housed locally and cannot be installed via `pip` or another package manager. Therefore, it must be downloaded/cloned from GitHub. I have included `scoring_module.py` as part of a $\\texttt{git}$ submodule in this repo. Let's bring `scoring_module.py` into the current directory so we can actually use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to check whether it already exists in the current working directory. To see the current working directory, type `pwd` in the terminal or run `!pwd` in a Python env. Below is a function which checks whether `scoring_module.py` exists in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def does_scoring_module_exist():\n",
    "    curr = os.path.abspath(os.path.dirname(__name__)) # gets current path/directory\n",
    "    module = curr + '/Honors-Thesis/scoring/scoring_module.py' # string variable pointing to location of scoring_module.py\n",
    "\n",
    "    if os.path.exists(f'{curr}/scoring_module.py'): # check if scoring_module.py is already in current path/directory\n",
    "        print(f'scoring_module.py already in current directory\\n')\n",
    "    else: # if scoring_module.py is not in current path/directory, then add it \n",
    "        shutil.copy(module, curr)\n",
    "        print(f'scoring_module.py successfully added\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: an alternative to `curr = os.path.abspath(os.path.dirname(__name__))` is as follows.\n",
    "```python\n",
    "from pathlib import Path\n",
    "curr = Path.cwd() # gets current path/directory\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's call `does_scoring_module_exist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring_module.py succsefully added\n",
      "\n"
     ]
    }
   ],
   "source": [
    "does_scoring_module_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, `scoring_module.py`, has now been added to a place where you and this notebook can easily access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does `scoring_module.py` consist of?\n",
    "\n",
    "#### `scoring_module.py` is written in an object-oriented fashion. There are two classes, `Data` and `Stats`. The former cleans and prepares the data for the latter to compute the correct statistical tests mentioned in a [previous section](#more-on-the-behavioral-assessment). Overall, `scoring_module.py` is for file I/O, data cleaning and simple statistical analysis. Let's see what's actually in `scoring_module.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "# imports for this module\n",
      "import pandas as pd\n",
      "import os \n",
      "from scipy.stats import ttest_1samp, ttest_ind\n",
      "\n",
      "class Data(object):\n",
      "    \"\"\"Class for getting all the files you wish to analyze and putting them in a single object\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path: str, default: current path\n",
      "        Absolute path to the folder which holds data files.\n",
      "        Must be in .csv format\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, path=os.path.dirname(os.path.abspath(__name__))):\n",
      "        self.path = path\n",
      "        if os.path.isdir(path):\n",
      "            self.files = os.listdir(self.path)\n",
      "        elif os.path.isfile(path):\n",
      "            self.files = path\n",
      "\n",
      "    def parse_files(self, subids):\n",
      "        \"\"\"Find all of the files you wish to score\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "            List of subject IDs that match the filenames. \n",
      "            Example: subids = ['sub-001', 'sub-002', 'sub-003']\n",
      "        \"\"\"\n",
      "        \n",
      "        files = []\n",
      "        for id in subids:\n",
      "            found = False\n",
      "            for filename in self.files:\n",
      "                filename2 = filename.split('_')[0]\n",
      "                if id == filename2 and filename.endswith('.csv'):\n",
      "                    files.append(os.path.join(self.path, filename))\n",
      "                    found = True\n",
      "                    break  # No need to continue checking if the file is found\n",
      "            if not found:\n",
      "                print(f'No files found for subject ID {id}')\n",
      "        \n",
      "        # return\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n",
      "    def rm_subs(self, subids):\n",
      "        \"\"\"Remove subjects' files from object\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "             List of subject IDs that match the filenames. \n",
      "        \"\"\"\n",
      "\n",
      "        # bring in current files\n",
      "        files = self.files\n",
      "\n",
      "        # get sub ids\n",
      "        subids = subids\n",
      "\n",
      "        # remove requested files\n",
      "        _2rm = []\n",
      "        for file in files:\n",
      "            for id in subids:\n",
      "                if file.split('/')[-1].split('_')[0] == id:\n",
      "                    _2rm.append(file)\n",
      "\n",
      "        [files.remove(i) for i in _2rm]\n",
      "\n",
      "        # return new\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n",
      "\n",
      "    def clean(self):\n",
      "        \"\"\"Remove erroneous columns from .csv file generated from PsychoPy\"\"\"\n",
      "\n",
      "        # remove unnecessary columns \n",
      "        label = ['example_outer_loop.thisRepN',\n",
      "        'example_outer_loop.thisTrialN',\n",
      "        'example_outer_loop.thisN',\n",
      "        'example_outer_loop.thisIndex',\n",
      "        'example_inner_loop.thisRepN',\n",
      "        'example_inner_loop.thisTrialN',\n",
      "        'example_inner_loop.thisN',\n",
      "        'example_inner_loop.thisIndex',\n",
      "        'example_shift_loop.thisRepN',\n",
      "        'example_shift_loop.thisTrialN',\n",
      "        'example_shift_loop.thisN',\n",
      "        'example_shift_loop.thisIndex',\n",
      "        'trials_loop.thisRepN',\n",
      "        'trials_loop.thisTrialN',\n",
      "        'trials_loop.thisN',\n",
      "        'trials_loop.thisIndex',\n",
      "        'transition_loop.thisRepN',\n",
      "        'transition_loop.thisTrialN',\n",
      "        'transition_loop.thisN',\n",
      "        'transition_loop.thisIndex',\n",
      "        'blocks_loop.thisRepN',\n",
      "        'blocks_loop.thisTrialN',\n",
      "        'blocks_loop.thisN',\n",
      "        'blocks_loop.thisIndex',\n",
      "        'word_loop.thisRepN',\n",
      "        'word_loop.thisTrialN',\n",
      "        'word_loop.thisN',\n",
      "        'word_loop.thisIndex',\n",
      "        'replay_msg_loop.thisRepN',\n",
      "        'replay_msg_loop.thisTrialN',\n",
      "        'replay_msg_loop.thisN',\n",
      "        'replay_msg_loop.thisIndex',\n",
      "        'instructions1_key.keys',\n",
      "        'instructions1_key.rt',\n",
      "        'instructions2_key.keys',\n",
      "        'instructions2_key.rt',\n",
      "        'instructions3_key.keys',\n",
      "        'instructions3_key.rt',\n",
      "        'instructions4_key.keys',\n",
      "        'instructions4_key.rt',\n",
      "        'shift1.started',\n",
      "        'shift1.stopped',\n",
      "        'shift2_2.started',\n",
      "        'shift2_2.stopped',\n",
      "        'instructions5_key.keys',\n",
      "        'instructions5_key.rt',\n",
      "        'word_sound.started',\n",
      "        'word1_shape.started',\n",
      "        'word_sound.stopped',\n",
      "        'word1_shape.stopped',\n",
      "        'jitter_shape.started',\n",
      "        'jitter_shape.stopped',\n",
      "        # 'shift2_shape.started',\n",
      "        # 'shift2_shape.stopped',\n",
      "        'replay_msg_text.started',\n",
      "        'response_text.started',\n",
      "        'key_resp.started',\n",
      "        'replay_msg_text.stopped',\n",
      "        'participant',\n",
      "        'order',\n",
      "        'date',\n",
      "        'expName',\n",
      "        'psychopyVersion',\n",
      "        'frameRate']\n",
      "\n",
      "        # read in the datafile\n",
      "        for i in self.files:\n",
      "            df = pd.read_csv(i)\n",
      "            try:\n",
      "                df = df.drop(columns=label)\n",
      "                df.to_csv(i)\n",
      "            except:\n",
      "                KeyError\n",
      "\n",
      "        return self\n",
      "\n",
      "    def score(self):\n",
      "        \"\"\"Scoring individual participant data\"\"\"\n",
      "\n",
      "        # create empty df for scoring\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        str_scores = []\n",
      "        rand_scores = []\n",
      "\n",
      "        for i in self.files:\n",
      "\n",
      "            # read in the file\n",
      "            df = pd.read_csv(i)\n",
      "\n",
      "            order = []\n",
      "            if df['blocks'][5] == 'block1.csv':\n",
      "                order = 1\n",
      "            elif df['blocks'][5] == 'block12.csv':\n",
      "                order = 2\n",
      "\n",
      "            anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "            # set answer key based on order set a few chunks earlier\n",
      "            answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "            if order == 1:\n",
      "                anskey['correct_key_resp'] = answers\n",
      "            elif order == 2:\n",
      "                anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "            for j in range(12):\n",
      "                if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                    anskey['assign_codes'][j] = 1\n",
      "                else:\n",
      "                    anskey['assign_codes'][j] = 0\n",
      "            # get score\n",
      "            anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "            score = anskey['score'][0]\n",
      "        \n",
      "            fn = i.split('/')[-1] # filename\n",
      "            cond = fn.split('_')[1] #.split('.')[0] #condition\n",
      "\n",
      "            if cond == 'structured':\n",
      "                str_scores.append(score)\n",
      "            elif cond == 'random':\n",
      "                rand_scores.append(score)\n",
      "\n",
      "        scores = pd.DataFrame(columns=['structured', 'random'])\n",
      "        try:\n",
      "            scores['structured'] = str_scores\n",
      "            scores['random'] = rand_scores\n",
      "        except:\n",
      "            ValueError\n",
      "\n",
      "        self.anskey = anskey\n",
      "        self.scores = scores\n",
      "\n",
      "        return self\n",
      "    \n",
      "    def indiv_score(self, subid):\n",
      "        \"\"\"Get score and individual responses for a single subject\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subid: str\n",
      "            single subject ID that matches filename\n",
      "        \"\"\"\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        file = []\n",
      "        for filename in self.files:\n",
      "            if filename.split('/')[-1].split('_')[0] == subid:\n",
      "                file.append(filename)\n",
      "                \n",
      "        df = pd.read_csv(file[0])\n",
      "\n",
      "        order = []\n",
      "        if df['blocks'][5] == 'block1.csv':\n",
      "            order = 1\n",
      "        elif df['blocks'][5] == 'block12.csv':\n",
      "            order = 2\n",
      "\n",
      "        anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "        # set answer key based on order set a few chunks earlier\n",
      "        answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "        if order == 1:\n",
      "            anskey['correct_key_resp'] = answers\n",
      "        elif order == 2:\n",
      "            anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "        for j in range(12):\n",
      "            if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                anskey['assign_codes'][j] = 1\n",
      "            else:\n",
      "                anskey['assign_codes'][j] = 0\n",
      "        # get score\n",
      "        anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "        \n",
      "        return anskey\n",
      "\n",
      "        \n",
      "\n",
      "class Stats(Data):\n",
      "    \"\"\"Class to compute a 1 sample, 1 tailed t-test or two-samples independant, two tailed t-test\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    def __init__(self, scores):\n",
      "        super().__init__() # gets 'self' from previous class\n",
      "\n",
      "        self.scores=scores\n",
      "\n",
      "    def compute(self, test, **kwargs):\n",
      "        '''Method for calculating scores\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test: str or int\n",
      "            Choose which statistical test you want to computer. \n",
      "            1 or '1samp' = 1 sample, 1 tailed t-test\n",
      "            2 or 'ind' = two samples, independet, two-tailed t-test\n",
      "\n",
      "        Keyword Arguments\n",
      "        -----------------\n",
      "        mu: int\n",
      "            Population parameter you wish to test agains. Only necessary if test = 1samp\n",
      "        '''\n",
      "                \n",
      "        if test == 1 or test == '1samp':\n",
      "            self.test = '1samp'    \n",
      "        elif test == 2 or test =='ind':\n",
      "            self.test = 'ind'\n",
      "\n",
      "        self.mu = kwargs.get('mu')\n",
      "        if self.mu is None and self.test == '1samp':\n",
      "            raise KeyError('When conducting a 1 sample t-test, you must provide the population parameter (mu). \\n +\\\n",
      "                           In the case of this project, mu= (1/3)')\n",
      "\n",
      "        if self.test == '1samp':\n",
      "             # compute test\n",
      "            statistic = ttest_1samp(self.scores['structured'], popmean=self.mu, alternative='greater')\n",
      "\n",
      "        elif self.test == 'ind':\n",
      "            # compute test\n",
      "            statistic = ttest_ind(a=self.scores['structured'], b=self.scores['random'], equal_var=True, alternative='greater')\n",
      "\n",
      "        self.statistic=statistic\n",
      "        return self\n"
     ]
    }
   ],
   "source": [
    "!cat scoring_module.py # the '!' operator allows you to execute unix/bash commands within a Python env. This particular one prints the contents of a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more in-depth look at `scoring_module.py`\n",
    "\n",
    "#### However, what we just did doesn't tell us anything important unless you're up for reading the entire module at once. For now, let's look at each class and its attributes and methods individually. To start, we must first import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoring_module as sm # import scoring_module.py and give it a call heuristic\n",
    "# alternatively...\n",
    "# from scoring_module import Data, Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've imported the module, let's look at `Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class for getting all the files you wish to analyze and putting them in a single object\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path: str, default: current path\n",
      "        Absolute path to the folder which holds data files.\n",
      "        Must be in .csv format\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(sm.Data.__doc__) # prints the docstring. Certain IDEs will allow easier access to this (e.g., VSCODE or PyCharm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What this is telling us is that `Data` requires a path locating the data. Let's do this! But first we need to get some data. Fortunately, there is some sample data in the $\\texttt{git}$ submodule. Let's move it over here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "curr = os.path.abspath(os.path.dirname(__name__)) # we've seen this before\n",
    "sample_data = curr + \"/Honors-Thesis/scoring/testdata/\" # pointer to data\n",
    "sample_data_dir = curr + \"/data_dir/\" # where to move data\n",
    "os.makedirs(sample_data_dir) # make the directory for the data to go to\n",
    "shutil.copytree(sample_data, sample_data_dir, dirs_exist_ok=True) # copies contents of subdir recursively to specified dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that we had to make a destination folder for this process, unlike `does_scoring_module_exist()`. This is because of the functionality of `shutil.copy` versus `shutil.copytree`. You can copy entire directories more easily in the terminal by following commands below.\n",
    "\n",
    "```bash\n",
    "cp -r ./SourceDir /.DestinationDir\n",
    "```\n",
    "\n",
    "#### The folder containing the data is `data_dir`. Let's give it to `Data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr = os.path.abspath(os.path.dirname(__name__)) # we've seen this before\n",
    "data_dir_pointer = curr + \"/data_dir/\"\n",
    "data = sm.Data(path=data_dir_pointer) # assigning the object to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what happens next? Well, we need to use the *methods* within the `Data` class to clean and prep the data for the necessary statistical tests. To see which methods belong to the data class, let's run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean', 'indiv_score', 'parse_files', 'rm_subs', 'score']\n"
     ]
    }
   ],
   "source": [
    "Data_methods = [i for i in dir(sm.Data) if callable(getattr(sm.Data, i))] # This gets all of the methods and attributes of the Data class and appends it to a list\n",
    "\n",
    "# However, this list contains some unneccessary items, so lets remove them (comment out this conditional statement to see the extra stuff)\n",
    "[Data_methods.remove(i) for i in Data_methods[:] if i.startswith(\"__\")]\n",
    "\n",
    "# see what methods are in Data\n",
    "print(Data_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The methods are what the object can *do*. To see what is required of each method, you can call the docstring using the same steps to access the docstring for `Data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at what's under the hood of each method. To do this, we can use the `inspect` package, which comes preinstalled with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def parse_files(self, subids):\n",
      "        \"\"\"Find all of the files you wish to score\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "            List of subject IDs that match the filenames. \n",
      "            Example: subids = ['sub-001', 'sub-002', 'sub-003']\n",
      "        \"\"\"\n",
      "        \n",
      "        files = []\n",
      "        for id in subids:\n",
      "            found= False\n",
      "            for filename in self.files:\n",
      "                filename2 = filename.split('_')[0]\n",
      "                if id == filename2 and filename.endswith('.csv'):\n",
      "                    files.append(os.path.join(self.path, filename))\n",
      "                    found = True\n",
      "                    break\n",
      "                if not found:\n",
      "                    print('Looking for more files')\n",
      "        \n",
      "        # return\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource # This prints the source code of a specified function, class, method, etc\n",
    "\n",
    "print(getsource(sm.Data.parse_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you scroll to the top of this output, you can see the docstring, wrapped in tripple quotes `\"\"\" \"\"\"`. You can see that this method, `parse_files`, \"Find[s] all of the files you wish to score\". It does this by relying on a filenaming convention established in the project. You must provide a subject ID to the method. What this code does is iterate through every file in the path you originally gave the parent class and check whether it is a `.csv` file *and* contains the subject ID(s) you've provided. This is necessary because PsychoPy outputs many files that are not needed and this particular method helps ignore those. \n",
    "\n",
    "#### Let's call this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subid = [\"sub-001\"] # provide subject ID\n",
    "data.parse_files(subids=subid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is an attribute within the `Data` class which lets you look at the file(s) the program currently is holding/looking through. Let's look at what is currently in our `data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lendlab/Box Sync/willdecker/GitHub/GaTech_Code/data_dir/sub-001_structured_3afc.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above file is the exact file we provided the program. Now, we need to clean the file. As mentioned earlier, PsychoPy saves additional data that we will not be needing. To remove this, I have built a method called `clean`, which automatically removes these columns. Let's take a closer look at the source code for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def clean(self):\n",
      "        \"\"\"Remove erroneous columns from .csv file generated from PsychoPy\"\"\"\n",
      "\n",
      "        # remove unnecessary columns \n",
      "        label = ['example_outer_loop.thisRepN',\n",
      "        'example_outer_loop.thisTrialN',\n",
      "        'example_outer_loop.thisN',\n",
      "        'example_outer_loop.thisIndex',\n",
      "        'example_inner_loop.thisRepN',\n",
      "        'example_inner_loop.thisTrialN',\n",
      "        'example_inner_loop.thisN',\n",
      "        'example_inner_loop.thisIndex',\n",
      "        'example_shift_loop.thisRepN',\n",
      "        'example_shift_loop.thisTrialN',\n",
      "        'example_shift_loop.thisN',\n",
      "        'example_shift_loop.thisIndex',\n",
      "        'trials_loop.thisRepN',\n",
      "        'trials_loop.thisTrialN',\n",
      "        'trials_loop.thisN',\n",
      "        'trials_loop.thisIndex',\n",
      "        'transition_loop.thisRepN',\n",
      "        'transition_loop.thisTrialN',\n",
      "        'transition_loop.thisN',\n",
      "        'transition_loop.thisIndex',\n",
      "        'blocks_loop.thisRepN',\n",
      "        'blocks_loop.thisTrialN',\n",
      "        'blocks_loop.thisN',\n",
      "        'blocks_loop.thisIndex',\n",
      "        'word_loop.thisRepN',\n",
      "        'word_loop.thisTrialN',\n",
      "        'word_loop.thisN',\n",
      "        'word_loop.thisIndex',\n",
      "        'replay_msg_loop.thisRepN',\n",
      "        'replay_msg_loop.thisTrialN',\n",
      "        'replay_msg_loop.thisN',\n",
      "        'replay_msg_loop.thisIndex',\n",
      "        'instructions1_key.keys',\n",
      "        'instructions1_key.rt',\n",
      "        'instructions2_key.keys',\n",
      "        'instructions2_key.rt',\n",
      "        'instructions3_key.keys',\n",
      "        'instructions3_key.rt',\n",
      "        'instructions4_key.keys',\n",
      "        'instructions4_key.rt',\n",
      "        'shift1.started',\n",
      "        'shift1.stopped',\n",
      "        'shift2_2.started',\n",
      "        'shift2_2.stopped',\n",
      "        'instructions5_key.keys',\n",
      "        'instructions5_key.rt',\n",
      "        'word_sound.started',\n",
      "        'word1_shape.started',\n",
      "        'word_sound.stopped',\n",
      "        'word1_shape.stopped',\n",
      "        'jitter_shape.started',\n",
      "        'jitter_shape.stopped',\n",
      "        # 'shift2_shape.started',\n",
      "        # 'shift2_shape.stopped',\n",
      "        'replay_msg_text.started',\n",
      "        'response_text.started',\n",
      "        'key_resp.started',\n",
      "        'replay_msg_text.stopped',\n",
      "        'participant',\n",
      "        'order',\n",
      "        'date',\n",
      "        'expName',\n",
      "        'psychopyVersion',\n",
      "        'frameRate']\n",
      "\n",
      "        # read in the datafile\n",
      "        for i in self.files:\n",
      "            df = pd.read_csv(i)\n",
      "            try:\n",
      "                df = df.drop(columns=label)\n",
      "                df.to_csv(i)\n",
      "            except:\n",
      "                KeyError\n",
      "\n",
      "        return self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(sm.Data.clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see that this method, `clean` \"Remove[s] erroneous columns from .csv file generated from PsychoPy\". It does this by creating a list of the \"to-be deleted\" column headers and assigning it to the variables `labels`.\n",
    "\n",
    "#### Using `pandas` (a powerful and widely used Python library), it drops the columns listed in the `labels` variable. Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scoring_module.Data at 0x10f1437a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can score the data. Let's look at what the `score` method does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def score(self):\n",
      "        \"\"\"Scoring individual participant data\"\"\"\n",
      "\n",
      "        # create empty df for scoring\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        str_scores = []\n",
      "        rand_scores = []\n",
      "\n",
      "        for i in self.files:\n",
      "\n",
      "            # read in the file\n",
      "            df = pd.read_csv(i)\n",
      "\n",
      "            order = []\n",
      "            if df['blocks'][5] == 'block1.csv':\n",
      "                order = 1\n",
      "            elif df['blocks'][5] == 'block12.csv':\n",
      "                order = 2\n",
      "\n",
      "            anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "            # set answer key based on order set a few chunks earlier\n",
      "            answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "            if order == 1:\n",
      "                anskey['correct_key_resp'] = answers\n",
      "            elif order == 2:\n",
      "                anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "            for j in range(12):\n",
      "                if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                    anskey['assign_codes'][j] = 1\n",
      "                else:\n",
      "                    anskey['assign_codes'][j] = 0\n",
      "            # get score\n",
      "            anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "            score = anskey['score'][0]\n",
      "        \n",
      "            fn = i.split('/')[-1] # filename\n",
      "            cond = fn.split('_')[1] #.split('.')[0] #condition\n",
      "\n",
      "            if cond == 'structured':\n",
      "                str_scores.append(score)\n",
      "            elif cond == 'random':\n",
      "                rand_scores.append(score)\n",
      "\n",
      "        scores = pd.DataFrame(columns=['structured', 'random'])\n",
      "        try:\n",
      "            scores['structured'] = str_scores\n",
      "            scores['random'] = rand_scores\n",
      "        except:\n",
      "            ValueError\n",
      "\n",
      "        self.anskey = anskey\n",
      "        self.scores = scores\n",
      "\n",
      "        return self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(sm.Data.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method is a little more involved. Let's take it step by step.\n",
    "\n",
    "1. #### First, an empty `pandas` data frame, named `anskey` is created and two empty vectors, `str_scores` and `rand_scores` are initialized. \n",
    "2. #### Next, a `for` loop is initialized which iterates the within code over every file in `data.files`. \n",
    "#### Within this loop... \n",
    "* #### The `.csv` file is loaded.\n",
    "* #### An empty vector, `order`, is created and a conditional `if` statement looks at `data.files[i]` (the current file in the loop) and determines the order in which the stimuli were presented (either order 1 or 2). \n",
    "> For counterbalancing, some participants completed the task in which the order of the stimuli was completed in reverse relative to the others. \n",
    "* #### Next, the participant's actual responses are loaded into `anskey`. \n",
    "* #### Based on the defined order, as determined a few lines prior, the correct answers are flipped and assigned to another column in `anskey`.\n",
    "* #### Next, the code iterates between the correct answers and a participant's answers. If they match, a `1` is given to the participant. If they do not match (i.e., the participant incorrectly responded to a trial), then a `0` is given. These `1`'s and `0`'s are added to another column in `anskey`.\n",
    "* #### This new columns of `1`'s and `0`s is summed and divided by the total number of trials to derive an individual `score`.\n",
    "* #### Next, the filename, which contains the condition of a participant, determines whether this `score` is appended to `str_scores` or `rand_scores`.\n",
    "3. #### Following the loop, a new `pandas` dataframe, `scores`, is created. It adds all values from `str_scores` and `rand_scores` to specific columns.\n",
    "\n",
    "#### Let's score the data and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structured</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   structured random\n",
       "0    0.666667    NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.score() # executes method\n",
    "data.scores # allows us to view the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember, this code is written with the intent to handle multiple participant files. With only one sample dataset, the `scores` table is realtively small. We can also check a specific subject's responses using another method, `indiv_score`. Let's look at what this does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def indiv_score(self, subid):\n",
      "        \"\"\"Get score and individual responses for a single subject\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subid: str\n",
      "            single subject ID that matches filename\n",
      "        \"\"\"\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        file = []\n",
      "        for filename in self.files:\n",
      "            if filename.split('/')[-1].split('_')[0] == subid:\n",
      "                file.append(filename)\n",
      "                \n",
      "        df = pd.read_csv(file[0])\n",
      "\n",
      "        order = []\n",
      "        if df['blocks'][5] == 'block1.csv':\n",
      "            order = 1\n",
      "        elif df['blocks'][5] == 'block12.csv':\n",
      "            order = 2\n",
      "\n",
      "        anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "        # set answer key based on order set a few chunks earlier\n",
      "        answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "        if order == 1:\n",
      "            anskey['correct_key_resp'] = answers\n",
      "        elif order == 2:\n",
      "            anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "        for j in range(12):\n",
      "            if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                anskey['assign_codes'][j] = 1\n",
      "            else:\n",
      "                anskey['assign_codes'][j] = 0\n",
      "        # get score\n",
      "        anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "        \n",
      "        return anskey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(sm.Data.indiv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see from the docstring, this code \"Get[s] score and individual responses for a single subject\". This can be beneficial for sanity checks or to review patterns of commonly missed items. `indiv_score`, requires a argument to the parameter, `subid=`. Here, you specify which particular subject you wish to view. From there, it pretty much runs the same code used in `score`, but outputs `anskey`. Let's run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_key_resp</th>\n",
       "      <th>actual_key_resp</th>\n",
       "      <th>assign_codes</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v</td>\n",
       "      <td>z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>z</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   correct_key_resp actual_key_resp assign_codes     score\n",
       "0                 z               m            0  0.666667\n",
       "1                 v               z            0       NaN\n",
       "2                 v               v            1       NaN\n",
       "3                 z               z            1       NaN\n",
       "4                 m               m            1       NaN\n",
       "5                 z               z            1       NaN\n",
       "6                 z               m            0       NaN\n",
       "7                 v               m            0       NaN\n",
       "8                 m               m            1       NaN\n",
       "9                 v               v            1       NaN\n",
       "10                v               v            1       NaN\n",
       "11                m               m            1       NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.indiv_score(subid='sub-001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also remove a specific subject from the `data.files` attribute with the method `rm_subs`, which takes an argument for the `subids` parameter. I won't actually do this for this particular example because there is only one dataset, but I will show you what the syntax for this looks like given three data sets. \n",
    "\n",
    "```py\n",
    "data.scores # here are the score of three subjects\n",
    "\tstructured\trandom\n",
    "0\t0.583333\tNaN\n",
    "1\t0.520000\tNaN\n",
    "2\t0.500000\tNaN\n",
    "\n",
    "subid = 'sub-002'\n",
    "data.rm_subs(subids=subid) # remove sub-002\n",
    "data.scores # now sub-002's score is no longer part of the object.\n",
    "\tstructured\trandom\n",
    "0\t0.583333\tNaN\n",
    "1\t0.500000\tNaN\n",
    "```\n",
    "#### Notice that there are no values in the `random` column of `data.scores`. This is because this example dataset consisted only of data from participants who listened to structured sequences in the scanner. \n",
    "\n",
    "#### Importantly, you must be cautious when using this method. I have not created functionality to quickly add a single subject back into the data. Let's look at what's under the hood of `rm_subs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def rm_subs(self, subids):\n",
      "        \"\"\"Remove subjects' files from object\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "             List of subject IDs that match the filenames. \n",
      "        \"\"\"\n",
      "\n",
      "        # bring in current files\n",
      "        files = self.files\n",
      "\n",
      "        # get sub ids\n",
      "        subids = subids\n",
      "\n",
      "        # remove requested files\n",
      "        _2rm = []\n",
      "        for file in files:\n",
      "            for id in subids:\n",
      "                if file.split('/')[-1].split('_')[0] == id:\n",
      "                    _2rm.append(file)\n",
      "\n",
      "        [files.remove(i) for i in _2rm]\n",
      "\n",
      "        # return new\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(sm.Data.rm_subs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's happening here is the program iterates over each file and determines whether one of them matches the subject you've specified to be removed. If it finds a match, it removes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at how we would statistically analyze this data. To do this, we can use the `Stats` class created in `scoring_module.py`, which relies on the heuristics of this dataset and experiment to smoothly compute the necessary statistical tests (mentioned in [a previous section](#more-on-the-behavioral-assessment)). Let's first check out what this class does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to compute a 1 sample, 1 tailed t-test or two-samples independant, two tailed t-test\n",
      "    \n",
      "\n",
      "     \n",
      " \n",
      "['clean', 'compute', 'indiv_score', 'parse_files', 'rm_subs', 'score']\n"
     ]
    }
   ],
   "source": [
    "Stats_methods = [i for i in dir(sm.Stats) if callable(getattr(sm.Stats, i))] # This gets all of the methods and attributes of the Stats class and appends it to a list\n",
    "\n",
    "# However, this list contains some unneccessary items, so lets remove them (comment out this conditional statement to see the extra stuff)\n",
    "[Stats_methods.remove(i) for i in Stats_methods[:] if i.startswith(\"__\")]\n",
    "\n",
    "# print class doc string and methods\n",
    "print(f\"\"\"{sm.Stats.__doc__} \\n \n",
    "{Stats_methods}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's particularly interesting about this output is that it contains the same methods from `Data` with the addition of `compute`. This is the result of *parent* and *child* classes. In this case, `Data` is the parent class while `Stats` is the child class. In doing this, `Stats` inherits methods and attributes initialized within the parent class. This is done fairly easily with one to two lines of code in a child class:\n",
    "\n",
    "```python\n",
    "...\n",
    "# Parent class\n",
    "class Parent(object)\n",
    "\n",
    "    def __init_(self):\n",
    "        ...\n",
    "\n",
    "# Child class\n",
    "class Child(Parent)\n",
    "\n",
    "    def __init__(self):\n",
    "     super().__init__() # access parent methods and attributes\n",
    "     ...\n",
    "```\n",
    "\n",
    "#### If you look at the source code for `Stats`, you will see the above synatx implemented. \n",
    "\n",
    "#### Now let's look at what `Stats` can do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to assign the object to a variable and provide it with `data.scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sm.Stats(scores=data.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can use the `compute` method to compute the tests we want. Let's look at the backend of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def compute(self, test, **kwargs):\n",
      "        '''Method for calculating scores\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test: str or int\n",
      "            Choose which statistical test you want to computer. \n",
      "            1 or '1samp' = 1 sample, 1 tailed t-test\n",
      "            2 or 'ind' = two samples, independet, two-tailed t-test\n",
      "\n",
      "        Keyword Arguments\n",
      "        -----------------\n",
      "        mu: int\n",
      "            Population parameter you wish to test agains. Only necessary if test = 1samp\n",
      "        '''\n",
      "                \n",
      "        if test == 1 or test == '1samp':\n",
      "            self.test = '1samp'    \n",
      "        elif test == 2 or test =='ind':\n",
      "            self.test = 'ind'\n",
      "\n",
      "        self.mu = kwargs.get('mu')\n",
      "        if self.mu is None and self.test == '1samp':\n",
      "            raise KeyError('When conducting a 1 sample t-test, you must provide the population parameter (mu). \\n +\\\n",
      "                           In the case of this project, mu= (1/3)')\n",
      "\n",
      "        if self.test == '1samp':\n",
      "             # compute test\n",
      "            statistic = ttest_1samp(self.scores['structured'], popmean=self.mu, alternative='greater')\n",
      "\n",
      "        elif self.test == '1samp':\n",
      "            # compute test\n",
      "            statistic = ttest_ind(a=self.scores['structured'], b=self.scores['random'], equal_var=False, alternative='two-sided')\n",
      "\n",
      "        self.statistic=statistic\n",
      "        return self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(sm.Stats.compute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, you must choose which test to compute (see docstring). That's it! If you wish to do a one-sample test, then you must also provide `mu=`. This is allowed via `**kwargs`, or *keyword arguments*. These are brought into the function as a `dict` datatype. The function then searches through the dict to see if any keywords are present.\n",
    "\n",
    "#### However, we don't have sufficient sample data to compute the necessary tests. Let's create some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # let's use pandas and numpy\n",
    "import numpy as np\n",
    "\n",
    "def make_some_fake_data(size: int, structured_mean: int or float, random_mean: int or float):\n",
    "    \"\"\"Generates some fake data mimicing the heuristic for scoring_module.Stats\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size: int\n",
    "        Number of participants\n",
    "\n",
    "    structured_mean: int or float\n",
    "        Arbitrary mean for structured group\n",
    "\n",
    "    random_mean: int or float\n",
    "        Arbitrary mean for random group\n",
    "    \"\"\"\n",
    "    # make empty data frame with correct column names\n",
    "    df = pd.DataFrame(columns=['structured', 'random'])\n",
    "\n",
    "    # generate some fake data based on input\n",
    "    df['structured'] = np.abs(np.random.normal(loc=structured_mean, size=size))\n",
    "    df['random'] = np.abs(np.random.normal(loc=random_mean, size=size))\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above code, I am using `numpy.random.normal` to create data which follows the Gaussian (normal) distribution that make parameteric statistics possible. This is possible by providing a mean for our fake data's categories. Note: this function, `make_some_fake_data()`, generates some fake data that does *not* adhere to the project's actual data. That is, it generates some data larger than 1.0, which is impossible for this experiment. However, for the sake of this example, I will ignore this.\n",
    "\n",
    "#### Let's execute this function and look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structured</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.225705</td>\n",
       "      <td>0.522353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.038718</td>\n",
       "      <td>0.147747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.554391</td>\n",
       "      <td>0.198052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.199868</td>\n",
       "      <td>1.919180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.050924</td>\n",
       "      <td>0.394973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.840782</td>\n",
       "      <td>0.391198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.723327</td>\n",
       "      <td>0.869757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.154398</td>\n",
       "      <td>1.521322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.247239</td>\n",
       "      <td>2.727532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.332281</td>\n",
       "      <td>0.239924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.490644</td>\n",
       "      <td>0.433228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.308819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.529084</td>\n",
       "      <td>2.347303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028095</td>\n",
       "      <td>1.685602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.166549</td>\n",
       "      <td>0.643316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.024447</td>\n",
       "      <td>1.265272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.626225</td>\n",
       "      <td>0.802183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.601745</td>\n",
       "      <td>0.250265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.107973</td>\n",
       "      <td>1.278441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.036226</td>\n",
       "      <td>1.749334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    structured    random\n",
       "0     1.225705  0.522353\n",
       "1     1.038718  0.147747\n",
       "2     0.554391  0.198052\n",
       "3     2.199868  1.919180\n",
       "4     2.050924  0.394973\n",
       "5     0.840782  0.391198\n",
       "6     0.723327  0.869757\n",
       "7     0.154398  1.521322\n",
       "8     0.247239  2.727532\n",
       "9     0.332281  0.239924\n",
       "10    1.490644  0.433228\n",
       "11    0.901879  0.308819\n",
       "12    0.529084  2.347303\n",
       "13    0.028095  1.685602\n",
       "14    1.166549  0.643316\n",
       "15    1.024447  1.265272\n",
       "16    1.626225  0.802183\n",
       "17    0.601745  0.250265\n",
       "18    1.107973  1.278441\n",
       "19    2.036226  1.749334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = make_some_fake_data(size=20, structured_mean=0.8, random_mean=0.455)\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have some data to pass to `stats.Compute`. But, because we made some new data, we need to reinitialize our object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sm.Stats(scores=fake_data) # reinitialize stats\n",
    "test = stats.compute(test=2) # compute test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And that's it! Now let's see the results. This can be accessed by an attribute, `statistics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=0.04102772424802848, pvalue=0.48374430309665295, df=38.0)\n"
     ]
    }
   ],
   "source": [
    "print(test.statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on this, it looks like there is no difference between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "#### Behavioral data has now been analyzed and ready for interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
