{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "#### Here, we will be looking at a portion of code which scores participant behavioral data collected during [Will Decker's](w-decker.github.io) honors thesis project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description\n",
    "\n",
    "#### The overall aim of this project is to characterize the spatiotemporal neural dynamics of statistical learning (SL), a neurocognitive mechanism critical for perceptual learning. Here, participants listen to either structured or un-structured phonological sequences made up of 12 syllables while laying in a MRI scanner. The structure of the sequence is determined by transitional probabilities (TP). \n",
    "\n",
    "#### In the strucutred sequence, three phonemes were paired together to make a \"word\", which is the base unit repeated throughout the sequence, whereas the un-structured sequence contained no \"words\" and all syllables were randomly played, thus the base unit being a single phoneme. This means that the TP across base units in the structured group is $\\frac{1}{3}$[^1] and within base units is $1$, while the TP of the base unit in the un-structured group is $\\frac{1}{11}$. Three decades of evidence have shown that humans are sensitive to these TP's and are able to segment continuous input using the strucutre as defined by the TPs. Below is an illustrative example of the TPs in the structured versus un-structured sequence.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Structured TPs\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "    t1((/pu/))-.->|1.0| t2((/bi/))-.->|1.0| t3((/ka/))-.->|0.33| t4((/di/))-.->|1.0| t5((/da/))-.->|1.0| t6((/bu/))\n",
    "    t1---|WORD|t3\n",
    "    t4---|WORD|t6\n",
    "    style t1 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t2 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t3 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t4 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t5 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t6 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Un-structured TPs\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "    t1((/pu/))-.->|0.09| t2((/bi/))-.->|0.09| t3((/ka/))-.->|0.09| t4((/di/))-.->|0.09| t5((/da/))-.->|0.09| t6((/bu/))\n",
    "    style t1 fill:#f45,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t2 fill:#69f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t3 fill:#f9f,stroke:#333,stroke-width:4px,color:#000\n",
    "    style t4 fill:#451,stroke:#333,stroke-width:4px,color:#fff\n",
    "    style t5 fill:#205,stroke:#333,stroke-width:4px,color:#fff\n",
    "    style t6 fill:#904,stroke:#333,stroke-width:4px,color:#000\n",
    "\n",
    "```\n",
    "\n",
    "#### Ultimately, using advanced computational techniques--such as a [Hidden Markov Model (HMM)](https://brainiak.org/tutorials/12-hmm/)--, I expect to uncover three distinct subprocess of SL in the brain: a perceptual, encoding and predictive process.[^2] This would provide a spatially detailed mechanistic account of SL and give credence to existing evidence positing that SL is compositional.[^3] However, we must confirm that participants actually learned the structure.\n",
    "\n",
    "#### Upon completion of the sequence in the scanner, participants exited the scanner and completed a test which assessed whether an individual learned the structure of the phonological sequence. This test is composed of a three-alternative force choiced task, in which a word from the structured sequence (dubbed \"target word\") is pitted against two other foil words, which have never been heard by the participant. It is the participants job to discriminate between the foils and target word by correctly selecting the target word. Below is an example trial.\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: Example trial\n",
    "---\n",
    "flowchart LR\n",
    "\n",
    "A[FOIL]-.-> B[TARGET]-.-> C[FOIL]\n",
    "\n",
    "style A fill:#025\n",
    "style B fill:#f45\n",
    "style C fill:#025\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "D[Each presentation is played sequentially with a total of 12 trials.]\n",
    "```\n",
    "\n",
    "#### The code used to determine whether participants reliably learned the structure is reviewed in this notebook. \n",
    "\n",
    " [^1]: The construction of the sequences was constrained such that individual units could not immediately repeat themselves. More on the algorithm used to create sequences and its implementation can be found [here](/Honors-Thesis/README.md).\n",
    " [^2]: Information on the HMM implementation can be found [here](https://github.com/w-decker/SNL23_plots/blob/main/plotting.ipynb).\n",
    " [^3]: See [Batterink & Paller (2017)](https://www.batterinklab.com/_files/ugd/a9b75d_53f0f5269f5942cb81105ef47c84dba5.pdf) and [Moser et al. (2021)](https://www.batterinklab.com/_files/ugd/a9b75d_ab33c519fa7a406e92e68369eacfce2f.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on the behavioral assessment\n",
    "\n",
    "#### As seen above, participants had to determine the target word amidst two foil words. To assess learning, I examined whether individuals who were exposed to the structured sequence perform above chance ($0.33$) using a one-tailed one sample t-test and whether this performance was significantly greater than those exposed to the un-structured sequence using a one-tailed independent samples t-test.\n",
    "\n",
    "#### Additionally, this experiment was run on PsychoPy; the resulting output is noisy and the actual results of the assessment must be extracted from a file containing highly erroneous (in our particular case) data.\n",
    "\n",
    "#### Therefore, to analyze participant's SL abilities, I created a custom module for specifically handling the PsychoPy behavioral output. This is located within the repo submodule. For the purposes of this notebook, I'll extract it and bring it over to the current path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the custom module\n",
    "\n",
    "#### The custom module, named `scoring_module.py`, is housed locally and cannot be installed via `pip` or another package manager. Therefore, it must be downloaded/cloned from GitHub. I have included `scoring_module.py` as part of a $\\texttt{git}$ submodule in this repo. Let's bring `scoring_module.py` into the current directory so we can run through it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need to check whether it already exists in the current working directory. To see the current working directory, type `pwd` in the terminal or run `!pwd` in a Python env. Below is a function which checks whether `scoring_module.py` exists in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def does_scoring_module_exist():\n",
    "    curr = os.path.abspath(os.path.dirname(__name__)) # gets current path/directory\n",
    "    module = curr + '/Honors-Thesis/scoring/scoring_module.py' # string variable pointing to location of scoring_module.py\n",
    "\n",
    "    if os.path.exists(f'{curr}/scoring_module.py'): # check if scoring_module.py is already in current path/directory\n",
    "        print(f'scoring_module.py already in current directory\\n')\n",
    "    else: # if scoring_module.py is not in current path/directory, then add it \n",
    "        shutil.copy(module, curr)\n",
    "        print(f'scoring_module.py succsefully added\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's call `does_scoring_module_exist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring_module.py succsefully added\n",
      "\n"
     ]
    }
   ],
   "source": [
    "does_scoring_module_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, `scoring_module.py`, has now been added to a place where you and this notebook can easily access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does `scoring_module.py` consist of?\n",
    "\n",
    "#### `scoring_module.py` is written in an object-oriented fashion. There are two classes, `Data` and `Stats`. The former cleans and prepares the data for the latter to compute the correct statistical tests mentioned in a [previous section](#more-on-the-behavioral-assessment). Let's see what's actually in `scoring_module.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "# imports for this module\n",
      "import pandas as pd\n",
      "import os \n",
      "from scipy.stats import ttest_1samp, ttest_ind\n",
      "\n",
      "class Data(object):\n",
      "    \"\"\"Class for getting all the files you wish to analyze and putting them in a single object\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path: str, default: current path\n",
      "        Absolute path to the folder which holds data files.\n",
      "        Must be in .csv format\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, path=os.path.dirname(os.path.abspath(__name__))):\n",
      "        self.path = path\n",
      "        self.files = os.listdir(self.path)\n",
      "\n",
      "    def parse_files(self, subids):\n",
      "        \"\"\"Find all of the files you wish to score\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "            List of subject IDs that match the filenames. \n",
      "            Example: subids = ['sub-001', 'sub-002', 'sub-003']\n",
      "        \"\"\"\n",
      "        \n",
      "        files = []\n",
      "        for id in subids:\n",
      "            found= False\n",
      "            for filename in self.files:\n",
      "                filename2 = filename.split('_')[0]\n",
      "                if id == filename2 and filename.endswith('.csv'):\n",
      "                    files.append(os.path.join(self.path, filename))\n",
      "                    found = True\n",
      "                    break\n",
      "                if not found:\n",
      "                    print('Looking for more files')\n",
      "        \n",
      "        # return\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n",
      "    def rm_subs(self, subids):\n",
      "        \"\"\"Remove subjects' files from object\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        subids: list, str\n",
      "             List of subject IDs that match the filenames. \n",
      "        \"\"\"\n",
      "\n",
      "        # bring in current files\n",
      "        files = self.files\n",
      "\n",
      "        # get sub ids\n",
      "        subids = subids\n",
      "\n",
      "        # remove requested files\n",
      "        _2rm = []\n",
      "        for file in files:\n",
      "            for id in subids:\n",
      "                if file.split('/')[-1].split('_')[0] == id:\n",
      "                    _2rm.append(file)\n",
      "\n",
      "        [files.remove(i) for i in _2rm]\n",
      "\n",
      "        # return new\n",
      "        self.files = files\n",
      "        self.numfiles = len(files)\n",
      "\n",
      "\n",
      "    def clean(self):\n",
      "        \"\"\"Remove erroneous columns from .csv file generated from PsychoPy\"\"\"\n",
      "\n",
      "        # remove unnecessary columns \n",
      "        label = ['example_outer_loop.thisRepN',\n",
      "        'example_outer_loop.thisTrialN',\n",
      "        'example_outer_loop.thisN',\n",
      "        'example_outer_loop.thisIndex',\n",
      "        'example_inner_loop.thisRepN',\n",
      "        'example_inner_loop.thisTrialN',\n",
      "        'example_inner_loop.thisN',\n",
      "        'example_inner_loop.thisIndex',\n",
      "        'example_shift_loop.thisRepN',\n",
      "        'example_shift_loop.thisTrialN',\n",
      "        'example_shift_loop.thisN',\n",
      "        'example_shift_loop.thisIndex',\n",
      "        'trials_loop.thisRepN',\n",
      "        'trials_loop.thisTrialN',\n",
      "        'trials_loop.thisN',\n",
      "        'trials_loop.thisIndex',\n",
      "        'transition_loop.thisRepN',\n",
      "        'transition_loop.thisTrialN',\n",
      "        'transition_loop.thisN',\n",
      "        'transition_loop.thisIndex',\n",
      "        'blocks_loop.thisRepN',\n",
      "        'blocks_loop.thisTrialN',\n",
      "        'blocks_loop.thisN',\n",
      "        'blocks_loop.thisIndex',\n",
      "        'word_loop.thisRepN',\n",
      "        'word_loop.thisTrialN',\n",
      "        'word_loop.thisN',\n",
      "        'word_loop.thisIndex',\n",
      "        'replay_msg_loop.thisRepN',\n",
      "        'replay_msg_loop.thisTrialN',\n",
      "        'replay_msg_loop.thisN',\n",
      "        'replay_msg_loop.thisIndex',\n",
      "        'instructions1_key.keys',\n",
      "        'instructions1_key.rt',\n",
      "        'instructions2_key.keys',\n",
      "        'instructions2_key.rt',\n",
      "        'instructions3_key.keys',\n",
      "        'instructions3_key.rt',\n",
      "        'instructions4_key.keys',\n",
      "        'instructions4_key.rt',\n",
      "        'shift1.started',\n",
      "        'shift1.stopped',\n",
      "        'shift2_2.started',\n",
      "        'shift2_2.stopped',\n",
      "        'instructions5_key.keys',\n",
      "        'instructions5_key.rt',\n",
      "        'word_sound.started',\n",
      "        'word1_shape.started',\n",
      "        'word_sound.stopped',\n",
      "        'word1_shape.stopped',\n",
      "        'jitter_shape.started',\n",
      "        'jitter_shape.stopped',\n",
      "        # 'shift2_shape.started',\n",
      "        # 'shift2_shape.stopped',\n",
      "        'replay_msg_text.started',\n",
      "        'response_text.started',\n",
      "        'key_resp.started',\n",
      "        'replay_msg_text.stopped',\n",
      "        'participant',\n",
      "        'order',\n",
      "        'date',\n",
      "        'expName',\n",
      "        'psychopyVersion',\n",
      "        'frameRate']\n",
      "\n",
      "        # read in the datafile\n",
      "        for i in self.files:\n",
      "            df = pd.read_csv(i)\n",
      "            try:\n",
      "                df = df.drop(columns=label)\n",
      "                df.to_csv(i)\n",
      "            except:\n",
      "                KeyError\n",
      "\n",
      "        return self\n",
      "\n",
      "    def score(self):\n",
      "        \"\"\"Scoring individual participant data\"\"\"\n",
      "\n",
      "        # create empty df for scoring\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        str_scores = []\n",
      "        rand_scores = []\n",
      "\n",
      "        for i in self.files:\n",
      "\n",
      "            # read in the file\n",
      "            df = pd.read_csv(i)\n",
      "\n",
      "            order = []\n",
      "            if df['blocks'][5] == 'block1.csv':\n",
      "                order = 1\n",
      "            elif df['blocks'][5] == 'block12.csv':\n",
      "                order = 2\n",
      "\n",
      "            anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "            # set answer key based on order set a few chunks earlier\n",
      "            answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "            if order == 1:\n",
      "                anskey['correct_key_resp'] = answers\n",
      "            elif order == 2:\n",
      "                anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "            for j in range(12):\n",
      "                if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                    anskey['assign_codes'][j] = 1\n",
      "                else:\n",
      "                    anskey['assign_codes'][j] = 0\n",
      "            # get score\n",
      "            anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "            score = anskey['score'][0]\n",
      "        \n",
      "            fn = i.split('/')[-1] # filename\n",
      "            cond = fn.split('_')[1].split('.')[0] #condition\n",
      "\n",
      "            if cond == 'structured':\n",
      "                str_scores.append(score)\n",
      "            elif cond == 'random':\n",
      "                rand_scores.append(score)\n",
      "\n",
      "        scores = pd.DataFrame(columns=['structured', 'random'])\n",
      "        try:\n",
      "            scores['structured'] = str_scores\n",
      "            scores['random'] = rand_scores\n",
      "        except:\n",
      "            ValueError\n",
      "\n",
      "        self.anskey = anskey\n",
      "        self.scores = scores\n",
      "\n",
      "        return self\n",
      "    \n",
      "    def indiv_score(self, subid):\n",
      "        \"\"\"Get score and individual responses for a single subject\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subid: str\n",
      "            single subject ID that matches filename\n",
      "        \"\"\"\n",
      "        anskey = pd.DataFrame(columns=['correct_key_resp', 'actual_key_resp', 'assign_codes', 'score'])\n",
      "\n",
      "        file = []\n",
      "        for filename in self.files:\n",
      "            if filename.split('/')[-1].split('_')[0] == subid:\n",
      "                file.append(filename)\n",
      "                \n",
      "        df = pd.read_csv(file[0])\n",
      "\n",
      "        order = []\n",
      "        if df['blocks'][5] == 'block1.csv':\n",
      "            order = 1\n",
      "        elif df['blocks'][5] == 'block12.csv':\n",
      "            order = 2\n",
      "\n",
      "        anskey['actual_key_resp'] = list(df['key_resp.keys'][5:29].dropna())\n",
      "\n",
      "        # set answer key based on order set a few chunks earlier\n",
      "        answers =  ['z', 'v', 'v',  'z', 'm', 'z', 'z','v', 'm','v', 'v', 'm']\n",
      "        if order == 1:\n",
      "            anskey['correct_key_resp'] = answers\n",
      "        elif order == 2:\n",
      "            anskey['correct_key_resp'] = answers[::-1]\n",
      "\n",
      "        for j in range(12):\n",
      "            if anskey['correct_key_resp'][j] == anskey['actual_key_resp'][j]:\n",
      "                anskey['assign_codes'][j] = 1\n",
      "            else:\n",
      "                anskey['assign_codes'][j] = 0\n",
      "        # get score\n",
      "        anskey['score'][0] = (sum(anskey['assign_codes']))/12;\n",
      "        \n",
      "        return anskey\n",
      "\n",
      "        \n",
      "\n",
      "class Stats(Data):\n",
      "    \"\"\"Class to compute a 1 sample, 1 tailed t-test or two-samples independant, two tailed t-test\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    def __init__(self, scores):\n",
      "        super().__init__() # gets 'self' from previous class\n",
      "\n",
      "        self.scores=scores\n",
      "\n",
      "    def compute(self, test, **kwargs):\n",
      "        '''Method for calculating scores\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test: str or int\n",
      "            Choose which statistical test you want to computer. \n",
      "            1 or '1samp' = 1 sample, 1 tailed t-test\n",
      "            2 or 'ind' = two samples, independet, two-tailed t-test\n",
      "\n",
      "        Keyword Arguments\n",
      "        -----------------\n",
      "        mu: int\n",
      "            Population parameter you wish to test agains. Only necessary if test = 1samp\n",
      "        '''\n",
      "                \n",
      "        if test == 1 or test == '1samp':\n",
      "            self.test = '1samp'    \n",
      "        elif test == 2 or test =='ind':\n",
      "            self.test = 'ind'\n",
      "\n",
      "        self.mu = kwargs.get('mu')\n",
      "        if self.mu is None and self.test == '1samp':\n",
      "            raise KeyError('When conducting a 1 sample t-test, you must provide the population parameter (mu). \\n +\\\n",
      "                           In the case of this project, mu= (1/3)')\n",
      "\n",
      "        if self.test == '1samp':\n",
      "             # compute test\n",
      "            statistic = ttest_1samp(self.scores['structured'], popmean=self.mu, alternative='greater')\n",
      "\n",
      "        elif self.test == '1samp':\n",
      "            # compute test\n",
      "            statistic = ttest_ind(a=self.scores['structured'], b=self.scores['random'], equal_var=False, alternative='two-sided')\n",
      "\n",
      "        self.statistic=statistic\n",
      "        return self\n"
     ]
    }
   ],
   "source": [
    "!cat scoring_module.py # the '!' operator allows you to execute unix/bash commands within a Python env. This particular one prints the contents of a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, what we just did doesn't tell us anything important unless you're up for reading the entire module. For now, look at each class and its attributes and methods individually. To start, let's import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoring_module as sm # import scoring_module.py and give it a call heuristic\n",
    "# alternatively...\n",
    "# from scoring_module import Data, Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've imported the module, let's look at `Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class for getting all the files you wish to analyze and putting them in a single object\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path: str, default: current path\n",
      "        Absolute path to the folder which holds data files.\n",
      "        Must be in .csv format\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(sm.Data.__doc__) # prints the docstring. Certain IDEs will allow easier access to this (e.g., VSCODE or PyCharm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What this is telling us is that `Data` requires a path pointing the object to the data. Let's do this! First we need to get some data. Fortunately, there is some sample data in the $\\texttt{git}$ submodule. Let's move it over here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lendlab/Box Sync/willdecker/GitHub/GaTech_Code'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil \n",
    "curr = os.path.abspath(os.path.dirname(__name__))\n",
    "sample_data = curr + \"/Honors-Thesis/scoring/testdata/\"\n",
    "shutil.copytree(sample_data, curr, dirs_exist_ok=True) # copies contents of subdir recursively to parent dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The filename is `sub-001-3afc.csv`. Let's give it to `Data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sm.Data(path=\"sub-001-3afc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what happens next? Well, we need to use the *methods* within the `Data` class to clean and prep the data for the necessary statistical tests. To see which methods belong to the data class, let's run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean', 'indiv_score', 'parse_files', 'rm_subs', 'score']\n"
     ]
    }
   ],
   "source": [
    "Data_methods = [i for i in dir(sm.Data) if callable(getattr(sm.Data, i))] # This gets all of the methods and attributes of the Data class and appends it to a list\n",
    "\n",
    "# However, this list contains some unneccessary items, so lets remove them (comment out this conditional statement to see the extra stuff)\n",
    "[Data_methods.remove(i) for i in Data_methods[:] if i.startswith(\"__\")]\n",
    "\n",
    "# see what methods are in Data\n",
    "print(Data_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The methods are what the object can *do*. To see what is required of each method, you can call the docstring using the same steps to access the docstring for `Data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at what's under the hood of each method. To do this, we can use the `inspect` package, which comes preinstalled with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def clean(self):\n",
      "        \"\"\"Remove erroneous columns from .csv file generated from PsychoPy\"\"\"\n",
      "\n",
      "        # remove unnecessary columns \n",
      "        label = ['example_outer_loop.thisRepN',\n",
      "        'example_outer_loop.thisTrialN',\n",
      "        'example_outer_loop.thisN',\n",
      "        'example_outer_loop.thisIndex',\n",
      "        'example_inner_loop.thisRepN',\n",
      "        'example_inner_loop.thisTrialN',\n",
      "        'example_inner_loop.thisN',\n",
      "        'example_inner_loop.thisIndex',\n",
      "        'example_shift_loop.thisRepN',\n",
      "        'example_shift_loop.thisTrialN',\n",
      "        'example_shift_loop.thisN',\n",
      "        'example_shift_loop.thisIndex',\n",
      "        'trials_loop.thisRepN',\n",
      "        'trials_loop.thisTrialN',\n",
      "        'trials_loop.thisN',\n",
      "        'trials_loop.thisIndex',\n",
      "        'transition_loop.thisRepN',\n",
      "        'transition_loop.thisTrialN',\n",
      "        'transition_loop.thisN',\n",
      "        'transition_loop.thisIndex',\n",
      "        'blocks_loop.thisRepN',\n",
      "        'blocks_loop.thisTrialN',\n",
      "        'blocks_loop.thisN',\n",
      "        'blocks_loop.thisIndex',\n",
      "        'word_loop.thisRepN',\n",
      "        'word_loop.thisTrialN',\n",
      "        'word_loop.thisN',\n",
      "        'word_loop.thisIndex',\n",
      "        'replay_msg_loop.thisRepN',\n",
      "        'replay_msg_loop.thisTrialN',\n",
      "        'replay_msg_loop.thisN',\n",
      "        'replay_msg_loop.thisIndex',\n",
      "        'instructions1_key.keys',\n",
      "        'instructions1_key.rt',\n",
      "        'instructions2_key.keys',\n",
      "        'instructions2_key.rt',\n",
      "        'instructions3_key.keys',\n",
      "        'instructions3_key.rt',\n",
      "        'instructions4_key.keys',\n",
      "        'instructions4_key.rt',\n",
      "        'shift1.started',\n",
      "        'shift1.stopped',\n",
      "        'shift2_2.started',\n",
      "        'shift2_2.stopped',\n",
      "        'instructions5_key.keys',\n",
      "        'instructions5_key.rt',\n",
      "        'word_sound.started',\n",
      "        'word1_shape.started',\n",
      "        'word_sound.stopped',\n",
      "        'word1_shape.stopped',\n",
      "        'jitter_shape.started',\n",
      "        'jitter_shape.stopped',\n",
      "        # 'shift2_shape.started',\n",
      "        # 'shift2_shape.stopped',\n",
      "        'replay_msg_text.started',\n",
      "        'response_text.started',\n",
      "        'key_resp.started',\n",
      "        'replay_msg_text.stopped',\n",
      "        'participant',\n",
      "        'order',\n",
      "        'date',\n",
      "        'expName',\n",
      "        'psychopyVersion',\n",
      "        'frameRate']\n",
      "\n",
      "        # read in the datafile\n",
      "        for i in self.files:\n",
      "            df = pd.read_csv(i)\n",
      "            try:\n",
      "                df = df.drop(columns=label)\n",
      "                df.to_csv(i)\n",
      "            except:\n",
      "                KeyError\n",
      "\n",
      "        return self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource # This prints the source code of a specified function, class, method, etc\n",
    "print(getsource(sm.Data.clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you scroll to the top of this output, you can see the docstring, wrapped in tripple quotes `\"\"\" \"\"\"`. You can see that this method, `clean` \"Remove[s] erroneous columns from .csv file generated from PsychoPy\". It does this by creating a list of the \"to-be deleted\" column headers and assigning it to the variables `labels`.\n",
    "\n",
    "#### Using `pandas` (a powerful and widely used Python library), it drops the columns listed in the `labels` variable. Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lendlab/Box Sync/willdecker/GitHub/GaTech_Code/gatech_code.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lendlab/Box%20Sync/willdecker/GitHub/GaTech_Code/gatech_code.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49mclean()\n",
      "File \u001b[0;32m~/Box Sync/willdecker/GitHub/GaTech_Code/scoring_module.py:151\u001b[0m, in \u001b[0;36mData.clean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m# read in the datafile\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles:\n\u001b[0;32m--> 151\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(i)\n\u001b[1;32m    152\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mlabel)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 's'"
     ]
    }
   ],
   "source": [
    "data.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
